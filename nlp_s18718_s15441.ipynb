{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwzjrXyfSoo2",
        "outputId": "01ef2f71-6253-4ad4-bb92-bc813e434df3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-89ebe48d50cb>:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv('Book3.csv', encoding='latin-1', nrows=50000, error_bad_lines=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   sentiment          id                          date      flag  \\\n",
            "0          0  1548274671  Fri Apr 17 20:30:31 PDT 2009  NO_QUERY   \n",
            "1          0  1548274782  Fri Apr 17 20:30:34 PDT 2009  NO_QUERY   \n",
            "2          0  1548275152  Fri Apr 17 20:30:38 PDT 2009  NO_QUERY   \n",
            "3          0  1548275569  Fri Apr 17 20:30:39 PDT 2009  NO_QUERY   \n",
            "4          0  1548275799  Fri Apr 17 20:30:43 PDT 2009  NO_QUERY   \n",
            "\n",
            "              user                                         tweet_text  \n",
            "0     xoLovebug224  Working on my songg for aunt nan.   kinda hard...  \n",
            "1      Kerry_Baker  can't sleep  it's 4.30am and i have to be up a...  \n",
            "2  glamorusindie81  wishing i could be at coachella this weekend  ...  \n",
            "3            WOnet  Well   was having a tough day/night. Wanted  t...  \n",
            "4  jessicakornberg  taking some much needed naked time.  too bad i...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('Book3.csv', encoding='latin-1', nrows=50000, error_bad_lines=False)\n",
        "df.columns = [\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"tweet_text\"]\n",
        "\n",
        "# Convert sentiment 4 to 1\n",
        "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
        "\n",
        "# Preprocessing\n",
        "df = df.dropna(subset=['tweet_text', 'sentiment'])  # Drop rows with missing values\n",
        "\n",
        "# Function to clean tweets\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove urls\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)  # remove at and hashtag\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')  # remove non-ascii characters\n",
        "    return text\n",
        "\n",
        "df['tweet_text'] = df['tweet_text'].apply(clean_text)\n",
        "\n",
        "# Print the first few rows of the dataframe\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6khRAs8JSski",
        "outputId": "27e0da9b-a248-4ba4-a4e1-4089b917abcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Naive Bayes\n",
            "Confusion Matrix:\n",
            "[[2776 1321]\n",
            " [1069 4834]]\n",
            "Accuracy Score: 0.761\n",
            "ROC-AUC Score: 0.7482372970458753\n",
            "\n",
            "Model: SVM\n",
            "Confusion Matrix:\n",
            "[[2473 1624]\n",
            " [ 701 5202]]\n",
            "Accuracy Score: 0.7675\n",
            "ROC-AUC Score: 0.7424296114827826\n",
            "\n",
            "Model: Random Forest\n",
            "Confusion Matrix:\n",
            "[[2705 1392]\n",
            " [1154 4749]]\n",
            "Accuracy Score: 0.7454\n",
            "ROC-AUC Score: 0.7323726913554173\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Preprocessing\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['tweet_text'])  \n",
        "y = df['sentiment'] \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Modele\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Trening i ewaluacja\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Model: {name}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqBujkCaSv0i",
        "outputId": "ab8a56d1-379e-472e-d1b1-4a5b1ac9f903"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 507s 402ms/step - loss: 0.4961 - accuracy: 0.7593 - val_loss: 0.4542 - val_accuracy: 0.7855\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 487s 390ms/step - loss: 0.3945 - accuracy: 0.8224 - val_loss: 0.4563 - val_accuracy: 0.7897\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 485s 388ms/step - loss: 0.3352 - accuracy: 0.8521 - val_loss: 0.4751 - val_accuracy: 0.7800\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 485s 388ms/step - loss: 0.2848 - accuracy: 0.8740 - val_loss: 0.5445 - val_accuracy: 0.7822\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 483s 386ms/step - loss: 0.2395 - accuracy: 0.8968 - val_loss: 0.6534 - val_accuracy: 0.7768\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 479s 383ms/step - loss: 0.2019 - accuracy: 0.9142 - val_loss: 0.7003 - val_accuracy: 0.7700\n",
            "Epoch 6: early stopping\n",
            "313/313 [==============================] - 5455s 17s/step\n",
            "Confusion Matrix:\n",
            "[[   0 4097]\n",
            " [   0 5903]]\n",
            "Accuracy Score: 0.5903\n",
            "ROC-AUC Score: 0.5\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['tweet_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['tweet_text'])\n",
        "data = pad_sequences(sequences, maxlen=100)  # replace 100 with the length you want\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128))\n",
        "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "\n",
        "# Fiting\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[checkpoint, early_stop])\n",
        "\n",
        "# Wczytanie najlepszego modelu\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype('int32')\n",
        "\n",
        "# Ewaluacja\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGGy2RE7koc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "# Model\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(10000, 128))\n",
        "model_gru.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_gru = ModelCheckpoint('best_model_gru.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Fiting\n",
        "model_gru.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[checkpoint_gru])\n",
        "\n",
        "# Wczytanie najlepszego modelu\n",
        "model_gru.load_weights('best_model_gru.h5')\n",
        "\n",
        "y_pred_gru = (model_gru.predict(X_test) > 0.5).astype('int32')\n",
        "\n",
        "# Ewaluacja\n",
        "print(\"Confusion Matrix for GRU:\")\n",
        "print(confusion_matrix(y_test, y_pred_gru))\n",
        "print(\"Accuracy Score for GRU:\", accuracy_score(y_test, y_pred_gru))\n",
        "print(\"ROC-AUC Score for GRU:\", roc_auc_score(y_test, y_pred_gru))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_1lIQapnvZ",
        "outputId": "225b70ed-20f9-42be-f720-98e483d426ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIuiT1klS2Up"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(df['tweet_text'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "input_ids, X_test, y_train, y_test = train_test_split(inputs['input_ids'], y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(input_ids, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Zaczytanie Bert\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fine-tuning\n",
        "optimizer = create_optimizer(init_lr=1e-5, num_train_steps=5000, num_warmup_steps=500)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_bert_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Fiting\n",
        "model.fit([X_train, y_train], validation_data=([X_val, y_val]), epochs=3, batch_size=8, callbacks=[checkpoint])\n",
        "\n",
        "model.load_weights('best_bert_model.h5')\n",
        "\n",
        "y_pred = model.predict([X_test]).logits.argmax(axis=-1)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "PG6oc91_quMm",
        "outputId": "f4f00da3-59dc-4230-9ba8-3c8e80ab15f5"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "inputs = tokenizer(df['tweet_text'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "input_ids, X_test, y_train, y_test = train_test_split(inputs['input_ids'], y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(input_ids, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Zaczytanie RoBERTa\n",
        "model_roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "# Fine-tuning\n",
        "optimizer = create_optimizer(init_lr=1e-5, num_train_steps=5000, num_warmup_steps=500)\n",
        "model_roberta.compile(optimizer=optimizer, loss=model_roberta.compute_loss, metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_roberta_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Fiting\n",
        "model_roberta.fit([X_train, y_train], validation_data=([X_val, y_val]), epochs=3, batch_size=8, callbacks=[checkpoint])\n",
        "\n",
        "# Wczytanie najlepszego modelu\n",
        "model_roberta.load_weights('best_roberta_model.h5')\n",
        "\n",
        "y_pred = model_roberta.predict([X_test]).logits.argmax(axis=-1)\n",
        "\n",
        "# Ewaluacja\n",
        "print(\"Confusion Matrix for RoBERTa:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Accuracy Score for RoBERTa:\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC-AUC Score for RoBERTa:\", roc_auc_score(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
